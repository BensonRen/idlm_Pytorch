"""A module for a mixture density network layer

This file is copied from https://github.com/search?q=mixture+density+network+pytorch
MIT licensed

For more info on MDNs, see _Mixture Desity Networks_ by Bishop, 1994.
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torch.distributions import Categorical
import math


ONEOVERSQRT2PI = 1.0 / math.sqrt(2*math.pi)

class MDN(nn.Module):
    """A mixture density network layer

    The input maps to the parameters of a MoG probability distribution, where
    each Gaussian has O dimensions and diagonal covariance.

    Arguments:
        in_features (int): the number of dimensions in the input
        out_features (int): the number of dimensions in the output
        num_gaussians (int): the number of Gaussians per output dimensions

    Input:
        minibatch (BxD): B is the batch size and D is the number of input
            dimensions.

    Output:
        (pi, sigma, mu) (BxG, BxGxO, BxGxO): B is the batch size, G is the
            number of Gaussians, and O is the number of dimensions for each
            Gaussian. Pi is a multinomial distribution of the Gaussians. Sigma
            is the standard deviation of each Gaussian. Mu is the mean of each
            Gaussian.
    """
    def __init__(self, in_features, out_features, num_gaussians):
        super(MDN, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.num_gaussians = num_gaussians
        self.pi = nn.Sequential(
            nn.Linear(in_features, num_gaussians),
            nn.Softmax(dim=1)
        )
        self.sigma = nn.Linear(in_features, out_features*out_features*num_gaussians)
        self.mu = nn.Linear(in_features, out_features*num_gaussians)

    def forward(self, minibatch):
        #print('checki if there is nan in input:', torch.sum(torch.isnan(minibatch)))
        #print('input is: {}'.format(minibatch))
        pi = self.pi(minibatch)
        #print('in forward function, pi = {}'.format(pi))
        sigma = self.sigma(minibatch)
        sigma = sigma.view(-1, self.num_gaussians, self.out_features, self.out_features)
        #print('in forward function, sigma = {}'.format(sigma))
        mu = self.mu(minibatch)
        mu = mu.view(-1, self.num_gaussians, self.out_features)
        #print('in forward function, mu = {}'.format(mu))
        return pi, sigma, mu


def gaussian_probability(sigma, mu, target, eps=1e-5):
    """Returns the probability of `data` given MoG parameters `sigma` and `mu`.
    
    Arguments:
        sigma (BxGxO): The standard deviation of the Gaussians. B is the batch
            size, G is the number of Gaussians, and O is the number of
            dimensions per Gaussian.
        mu (BxGxO): The means of the Gaussians. B is the batch size, G is the
            number of Gaussians, and O is the number of dimensions per Gaussian.
        data (BxI): A batch of data. B is the batch size and I is the number of
            input dimensions.

    Returns:
        probabilities (BxG): The probability of each point in the probability
            of the distribution in the corresponding sigma/mu index.
    """
    eps = torch.tensor(eps, requires_grad=False)
    if torch.cuda.is_available():
        eps = eps.cuda()
    target = target.unsqueeze(1).expand_as(mu)
    #ll = torch.log(ONEOVERSQRT2PI / sigma) * (-0.5 * ((target - mu) / sigma)**2)
    #return torch.sum(ll, 2)
    ret = ONEOVERSQRT2PI * torch.exp(-0.5 * ((target - mu) / sigma)**2) / (sigma + eps)
    #ret = 0.5*torch.matmul((target - mu)
    return torch.max(torch.prod(ret, 2), eps)

def mdn_loss(pi, sigma, mu, target):
    """Calculates the error, given the MoG parameters and the target

    The loss is the negative log likelihood of the data given the MoG
    parameters.
    """
    #GP = gaussian_probability(sigma, mu, target)
    target = target.unsqueeze(1).expand_as(mu)
    loss = 0
    G = pi.size(1)
    B = pi.size(0)
    
    #########################
    # matrix implementation #
    #########################
    diff = target - mu
    print('size of diff = ', diff.size())
    print('size of sigma = ', sigma.size())
    mul1 = torch.matmul(diff, sigma)
    print('size of mul1 = ', mul1.size())
    print('size of diff_t = ', torch.transpose(diff, 1, 2))
    p_value =  torch.matmul(torch.matmul(diff, sigma),diff)# torch.transpose(diff))
    print('size of p_value = ', p_value.size())
    exit()
    #loss = torch.sum(torch.matmul(pi, p_value)



    ##########################
    # individual computation #
    ##########################
    for g in range(G):
        for b in range(B):
            diff = target[b, g, :] - mu[b, g, :]
            #print(diff.size())
            #print(sigma[:,g,:,:].size())
            p_value =  torch.matmul(torch.matmul(diff, sigma[b,g,:,:]),diff)# torch.transpose(diff))
            loss +=  0.5 * pi[b, g] * p_value                 # The diagonal part 
        loss += -torch.matmul(pi[:,g], torch.log(torch.sqrt(torch.det(sigma[:,g,:,:]))))
    print(loss.size())
    return loss
    #prob = pi*GP
    #prob = torch.log(pi)+ GP
    #print('pi part: {}, gaussian_part: {}'.format(pi, GP))
    #print('prob size = {}'.format(prob.size()))
    #for i in range(prob.size(1)):
    #    for j in range(prob.size(0)):
    #        print('prob {} = {}'.format(i, prob[j, i]))
    #print('sum(exp(prob)) = {}'.format(torch.sum(prob, dim=1)))
    #print('-log (sum(exp(prob)))={}'.format(-torch.log(torch.sum(prob, dim=1))))
    #print('mean = {}'.format(torch.mean(-torch.log(torch.sum(prob, dim=1)))))
    
    #nll =  -torch.log(torch.sum(prob, dim=1))
    #nll = -torch.sum(prob, dim=1)
    #nll = nll[torch.logical_not(torch.isinf(nll))]
    #nll = nll[torch.logical_not(torch.isnan(nll))]
    #print('mean = {}'.format(torch.mean(nll)))
    #return torch.mean(nll)


def sample(pi, sigma, mu):
    """Draw samples from a MoG.
    """
    categorical = Categorical(pi)
    pis = list(categorical.sample().data)
    sample = Variable(sigma.data.new(sigma.size(0), sigma.size(2)).normal_())
    for i, idx in enumerate(pis):
        sample[i] = sample[i].mul(sigma[i,idx]).add(mu[i,idx])
    return sample

def new_mdn_loss(pi, sigma, mu, target):
    """
    Copied from :
    https://github.com/sksq96/pytorch-mdn/blob/master/mdn.ipynb
    """
    m = torch.distributions.Normal(loc=mu, scale=sigma)
    loss = torch.exp(m.log_prob(target))
    loss = torch.sum(loss * pi, dim=1)
    loss = -torch.log(loss)
    return torch.mean(loss)
